{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3eXXjesAkEtc"
   },
   "source": [
    "# 11 - Processamento de Linguagem Natural\n",
    "\n",
    "Na aula de hoje, vamos explorar os seguintes tópicos em Python:\n",
    "\n",
    "- 1) Dados Estruturados e Não Estruturados\n",
    "- 2) Introdução a NLP\n",
    "- 3) Processamento de Textos\n",
    "- 4) Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NMDt8-hkEth"
   },
   "source": [
    "<img src=\"https://i1.wp.com/thedatascientist.com/wp-content/uploads/2018/09/data_science_wordcloud.png?fit=1584%2C1008&ssl=1\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "he7ufJQ0kEth"
   },
   "source": [
    "##   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZfxhkbnkEti"
   },
   "source": [
    "## Dados Estruturados e Não-Estruturados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9ELS2c8kEti"
   },
   "source": [
    "Primeiramente, precisamos entender qual a diferença enre as duas fontes de dados mais comuns, sendo elas dados **estruturados** e **não estruturados**. Definimos ele como:\n",
    "<br><br>\n",
    "- **Dados Estruturados:** São dados que seguem uma estrutura mais rígida com um padrão fixo e constante. Por exemplo: Tabelas e DataFrames;<br><br>\n",
    "- **Dados Não estruturados:** Como já diz o nome, são dados que não tem uma estrutura bem estabelecida e necessitam de um processamento adicional para trabalharmos com eles. Exemplos: áudios, vídeos, textos e etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwMWTWJfkEti"
   },
   "source": [
    "##   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Z3ao2GfkEtj"
   },
   "source": [
    "## Introdução ao Processamento de Linguagem Natural (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0i05o9GhkEtj"
   },
   "source": [
    "O Processamento de Linguagem Natural, mas conhecido como NLP, é a abordagem onde trabalhamos com **dados não estruturados** do tipo **Texto**. O objetivo de trabalharmos com textos é extrair de informação e teor linguístico das nossas bases de textos e converter isso de uma forma númerica, onde poderemos utilizar em nossos modelos de *Machine Learning*.<br><br>\n",
    "Temos como exemplos de aplicações de NLP como:\n",
    "- Análise de Sentimentos em review de filmes e produtos ou mensagens em redes sociais;\n",
    "- Filtro de E-Mails Spams e Não Spams;\n",
    "- Identificação de textos a partir de construções linguísticas (descobrir se um texto foi escrito ou não por Machado de assis);\n",
    "- Tradutores de Idiomas;\n",
    "- ChatBots;\n",
    "- Corretores Ortográficos;\n",
    "- Classificação de textos de acordo com o conteúdo do texto (Esportes, Política, Economia e etc).\n",
    "<br><br>\n",
    "Nesta aula iremos aprender a partir dos nossos dados textuais a como processar, tratar e transformar os dados de uma maneira que os modelos de *Machine Learning* entendam.<br><br>\n",
    "\n",
    "A principal biblioteca de referência para NLP chama-se [NLTK - Natural Language Toll Kit](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Cmxj1tdkEtk"
   },
   "source": [
    "##  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQ1b5Fb2kEtl"
   },
   "source": [
    "## Processamento de Textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve-57awxkEtl"
   },
   "source": [
    "Antes de mais nada, precisamos filtrar e tratar os nossos textos, de forma a deixar apenas o conteúdo de mais relevantes para a nossa análise. Existem alguns processos importantes para trabalhar com os textos (não necessariamente você precisa aplicar todos os procesos!), onde iremos detalhar a seguir:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMdBf90CkEtm"
   },
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Rj9A7DikEtm"
   },
   "source": [
    "Stopwords são palavras que aparecem com uma frequência muito alta nos textos, mas que não trazem um teor de conteúdo relevante para o nosso modelo. Vamos entender isso na prática:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 870,
     "status": "ok",
     "timestamp": 1642427358217,
     "user": {
      "displayName": "Alexandre Lima Santiago de Pinho",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08604687895817407219"
     },
     "user_tz": 180
    },
    "id": "AUqYZZ6XkEtm",
    "outputId": "d606cfce-42ee-4604-e086-f23a9a90b013"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\ITX\n",
      "[nltk_data]     Gamer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\ITX\n",
      "[nltk_data]     Gamer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEQjlpn1kEto"
   },
   "source": [
    "Baixada a função de Stopwords, vamos definir um set de stopwords onde teremos uma lista com todas as stopwords em inglês já identificadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bQ6CgvyNkEto"
   },
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1642097974608,
     "user": {
      "displayName": "Alexandre Lima Santiago de Pinho",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08604687895817407219"
     },
     "user_tz": 180
    },
    "id": "0Hgl5V0jkEtp",
    "outputId": "a64217f9-2169-40f4-c819-b2d6701fb428"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URgdGe6VkEtp"
   },
   "source": [
    "Vamos agora aplicar a remoção de Stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1642097975463,
     "user": {
      "displayName": "Alexandre Lima Santiago de Pinho",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08604687895817407219"
     },
     "user_tz": 180
    },
    "id": "kPMzBUgikEtp",
    "outputId": "806add15-2fe9-4137-d073-3f042730c54b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Lista original é:  ['my', 'house', 'is', 'black', 'and', 'white', 'but', \"isn't\", 'big']\n",
      "A Lista sem stopwords é:  ['house', 'black', 'white', 'big']\n"
     ]
    }
   ],
   "source": [
    "example = [\"my\", \"house\", \"is\", \"black\", \"and\", \"white\", \"but\", \"isn't\", \"big\"]\n",
    "\n",
    "clean_list = []\n",
    "\n",
    "for word in example:\n",
    "    if word not in stopwords:\n",
    "        clean_list.append(word)\n",
    "        \n",
    "print('A Lista original é: ', example)\n",
    "print('A Lista sem stopwords é: ', clean_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dlZ0feskEtq"
   },
   "source": [
    "##  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4lAgEtWkEtq"
   },
   "source": [
    "### Limpeza do Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtSDtWZFkEtq"
   },
   "source": [
    "Existem alguns cuidados com relação a grafia das palavras e elementos em um texto que devemos tomar bastante cuidado antes de fazer qualquer outra coisa. Esses pontos são:<br><br>\n",
    "- Transformar todas as palavras para MAIÚSCULAS ou minúsculas;\n",
    "- Remover caracteres especiais;\n",
    "- Remover dígitos (quando não forem relevantes);\n",
    "- Remover acentuação (caso típico de quando trabalhamos com textos em Português);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7musg23gkEtq"
   },
   "source": [
    "### Converter entre MAIÚSCULA e minúscula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1642097975467,
     "user": {
      "displayName": "Alexandre Lima Santiago de Pinho",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08604687895817407219"
     },
     "user_tz": 180
    },
    "id": "M0ijR_dBkEtq",
    "outputId": "0ac66573-9eb9-4739-cc41-5b901df9410d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavra Original:  OtoRriNoLaRINgOLoGIsTa\n",
      "Palavra Maiúscula:  OTORRINOLARINGOLOGISTA\n",
      "Palavra Minúscula:  otorrinolaringologista\n"
     ]
    }
   ],
   "source": [
    "string = 'OtoRriNoLaRINgOLoGIsTa'\n",
    "\n",
    "string_upper = string.upper()\n",
    "string_lower = string.lower()\n",
    "\n",
    "print('Palavra Original: ', string)\n",
    "print('Palavra Maiúscula: ', string_upper)\n",
    "print('Palavra Minúscula: ', string_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulaO-uSQkEtr"
   },
   "source": [
    "##  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AP9k-Pp1kEtr"
   },
   "source": [
    "### Remoção de dígitos, caracteres especiais e qualquer outro item que não queremos no texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnbn-RpHkEtr"
   },
   "source": [
    "Para essa etapa do processo, iremos utilizar uma biblioteca auxiliar [RegEx (Regular Expression)](https://docs.python.org/3/library/re.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HTbNgCdjkEtr"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SEOaDa7kEtr"
   },
   "source": [
    "Importada a biblioteca, vamos utilizar a função *re.sub*, para substituir os elementos que não queremos nos nossos textos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 246,
     "status": "ok",
     "timestamp": 1642427069352,
     "user": {
      "displayName": "Alexandre Lima Santiago de Pinho",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08604687895817407219"
     },
     "user_tz": 180
    },
    "id": "i6gwe4PJkEts",
    "outputId": "9640ee7a-7d3b-4961-c3df-07da6c763c91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto Original:  Siga nas redes sociais o @letscode, ja somos mais de 1 milhao de #hashtags e 200 mil followers\n",
      "Texto sem os números:  Siga nas redes sociais o @letscode, ja somos mais de  milhao de #hashtags e  mil followers\n",
      "Texto sem os caracteres especiais:  Siga nas redes sociais o letscode ja somos mais de milhao de hashtags e mil followers\n"
     ]
    }
   ],
   "source": [
    "string = 'Siga nas redes sociais o @letscode, ja somos mais de 1 milhao de #hashtags e 200 mil followers'\n",
    "print('Texto Original: ', string)\n",
    "\n",
    "# Remove Numbers\n",
    "string = re.sub(r'\\d', '', string)\n",
    "print('Texto sem os números: ', string)\n",
    "\n",
    "###\n",
    "# Remove Special Characters\n",
    "# ['agenda', 'agendado']\n",
    "# r\"agenda\" --> remove agenda, e fica o agendado\n",
    "# r\"agenda+\" --> remove ambos\n",
    "# r\"a+\" --> remove todas as palavras que começa com a\n",
    "# ^ --> complementar .: exemplo r\"agenda\" (remove a palavra agenda), r\"^agenda\" (remove todas as palavras diferente de agenda)\n",
    "string = re.sub(r\"[^a-zA-Z0-9]+\", ' ', string)\n",
    "print('Texto sem os caracteres especiais: ', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSL7gqb5kEts"
   },
   "source": [
    "Utilizem a documentação para descobrir mais códigos para filtrar elementos ou mesmo deem uma olhada nesse artigo, que resume de uma forma bem visual as aplicações do RegEx: [clique aqui](https://amitness.com/regex/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFVDaa1MkEts"
   },
   "source": [
    "##  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afM-nESCkEts"
   },
   "source": [
    "### Remoção de Acentuação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhbNxvhykEts"
   },
   "source": [
    "Para a remoção de acentuação, iremos utilizar uma bibloteca chamada *Unidecode*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4912,
     "status": "ok",
     "timestamp": 1642427256401,
     "user": {
      "displayName": "Alexandre Lima Santiago de Pinho",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08604687895817407219"
     },
     "user_tz": 180
    },
    "id": "ClNppf-ukEtt",
    "outputId": "9a95433f-dd30-405d-88c1-94fc3a63a8d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\programdata\\anaconda3\\lib\\site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "# Caso precise instalar a biblioteca, descomente o código abaixo\n",
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Uz3IO5CykEtt"
   },
   "outputs": [],
   "source": [
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 248,
     "status": "ok",
     "timestamp": 1642427267449,
     "user": {
      "displayName": "Alexandre Lima Santiago de Pinho",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08604687895817407219"
     },
     "user_tz": 180
    },
    "id": "KJ5IChBdkEtw",
    "outputId": "2385ac4b-5c0b-4460-947f-0701aabe623d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "João Sebastião Alvará Vovô Linguiça Expressão\n",
      "Joao Sebastiao Alvara Vovo Linguica Expressao\n"
     ]
    }
   ],
   "source": [
    "string = 'João Sebastião Alvará Vovô Linguiça Expressão'\n",
    "print(string)\n",
    "\n",
    "string = unidecode(string)\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvCnNxJZkEtw"
   },
   "source": [
    "##  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCcrfMCFkEtw"
   },
   "source": [
    "## Tokenização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bM24TuTwkEtw"
   },
   "source": [
    "Tokenização é um processo onde transformamos um texto de uma strin única em fragmentos desse texto na forma de *tokens*, que nada mais são do que as próprias palavras! Para isso, vamos utilizar a função *word_tokenize* do NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "s6-7TnSikEtw"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1642098010587,
     "user": {
      "displayName": "Alexandre Lima Santiago de Pinho",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08604687895817407219"
     },
     "user_tz": 180
    },
    "id": "uRxkmpvhkEtx",
    "outputId": "92b50ee9-4eda-445c-f5b9-0ce68879c1a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A frase original é:  O rato roeu a roupa do rei de Roma\n",
      "Os tokens são:  ['O', 'rato', 'roeu', 'a', 'roupa', 'do', 'rei', 'de', 'Roma']\n"
     ]
    }
   ],
   "source": [
    "string = 'O rato roeu a roupa do rei de Roma'\n",
    "\n",
    "words = word_tokenize(string)\n",
    "\n",
    "print('A frase original é: ', string)\n",
    "print('Os tokens são: ', words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uh7euNKmkEtx"
   },
   "source": [
    "##  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIfS2LfGkEtx"
   },
   "source": [
    "## Normalização de Textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_XO6rhFkEtx"
   },
   "source": [
    "**Normalização de Textos (Text Normalization)** é o procedimento que consiste em **padronizar** o texto, de modo a evitar que variações tornem os modelos demasiadamente complexos. Por exemplo: tratar singular/plural como a mesma coisa, ou então eliminar conjugação de verbos. Outras componentes comuns da normalização são a de eliminar palavras que não agregam muito significado, ou palavras muito raras.\n",
    "\n",
    "Abaixo alguns exemplos de ações de Text Normalization que podem ser aplicadas no pré-processamento de dados textuais:\n",
    "\n",
    "**Stemming** - Redução de tokens à sua raiz invariante através da **remoção de prefixos ou sufixos**. Baseado em heurística<br>\n",
    "**Lemmatization** - Redução de tokens à sua raiz invariante através da **análise linguística do token**. Baseado em dicionário léxico<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuOqOFr7kEtx"
   },
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "A2Yq2cAekEtx"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "zNIz7sYdkEty"
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "# http://snowball.tartarus.org/ suporte para mais de um idioma (inclusive portugues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 236,
     "status": "ok",
     "timestamp": 1642427334880,
     "user": {
      "displayName": "Alexandre Lima Santiago de Pinho",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08604687895817407219"
     },
     "user_tz": 180
    },
    "id": "NWkCZg6hkEty",
    "outputId": "7064e9d2-309d-442b-8d99-88aa6c9f6a19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['say', 'write', 'run', 'ate', 'work']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['saying', 'writing', 'running', 'ate', 'worked']\n",
    "\n",
    "stem_words = []\n",
    "for w in words:\n",
    "    s_words = stemmer.stem(w)\n",
    "    stem_words.append(s_words)\n",
    "    \n",
    "stem_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOgKIe3LkEty"
   },
   "source": [
    "##  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pz8xwd-EkEty"
   },
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 598,
     "status": "ok",
     "timestamp": 1642427368457,
     "user": {
      "displayName": "Alexandre Lima Santiago de Pinho",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08604687895817407219"
     },
     "user_tz": 180
    },
    "id": "Vtg_i-akkEty",
    "outputId": "5129cc54-2708-4839-8402-1506a6bab339"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\ITX\n",
      "[nltk_data]     Gamer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import these modules \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# na primeira vez, é necessário baixar o wordnet\n",
    "# após a primeira vez, pode comentar a linha abaixo\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "kcNAdaEakEtz"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2152,
     "status": "ok",
     "timestamp": 1642427384231,
     "user": {
      "displayName": "Alexandre Lima Santiago de Pinho",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08604687895817407219"
     },
     "user_tz": 180
    },
    "id": "GGjNtj44kEtz",
    "outputId": "f60cd5c6-d427-44cd-e527-d31fca78dc95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "running : run\n",
      "went : go\n"
     ]
    }
   ],
   "source": [
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
    "\n",
    "# argumento \"pos\" indica a qual classe gramatical o token pertence\n",
    "# pos = part of speech\n",
    "print(\"running :\", lemmatizer.lemmatize(\"running\", pos = \"v\")) \n",
    "print(\"went :\", lemmatizer.lemmatize(\"went\", pos = 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcWZJnyZkEtz"
   },
   "source": [
    "##   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W81ukvZXkEt0"
   },
   "source": [
    "## Pipeline de Processamento de Textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuFOySxckEt0"
   },
   "source": [
    "Conhecendo todos os tipos de processamentos que podemos utilizar, uma forma útil e organizada para isso é construirmos uma funçãi que receba o nosso dados originais e realizada todos os processamentos que queremos nos textos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "uuD_H082kEt0"
   },
   "outputs": [],
   "source": [
    "# Pipeline - Text Preprocessing\n",
    "def preprocessing(string):\n",
    "    ###\n",
    "    # Remove Numbers\n",
    "    string = re.sub(r'\\d', '', string)\n",
    "    ###\n",
    "    # Remove Special Characters\n",
    "    string = re.sub(r\"[^a-zA-Z0-9]+\", ' ', string)\n",
    "    ###\n",
    "    # Lowercase words\n",
    "    string = string.lower()\n",
    "    ###\n",
    "    # Word Tokenize\n",
    "    words = word_tokenize(string)\n",
    "    ###\n",
    "    # Remove Stopwords\n",
    "    filtered_words = []\n",
    "    for w in words:\n",
    "        if w not in stopwords:\n",
    "            filtered_words.append(w)\n",
    "    ###\n",
    "    # Stemming Words\n",
    "    stem_words = []\n",
    "    for w in filtered_words:\n",
    "        s_words = stemmer.stem(w)\n",
    "        stem_words.append(s_words)\n",
    "    ###\n",
    "    return stem_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXA4Vm5CkEt0"
   },
   "source": [
    "Vamos agora já começar a práticar com os nossos dados de exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nmj0hH22kEt0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byVUgMqQkEt0"
   },
   "outputs": [],
   "source": [
    "movies = pd.read_csv('/content/movies.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4D8EEHlYkEt0"
   },
   "source": [
    "Nosso exemplo será uma Análise de Sentimento em Críticas de Filmes, onde vamos identificar se a crítica foi boa ou não:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 265,
     "status": "ok",
     "timestamp": 1642098614988,
     "user": {
      "displayName": "Alexandre Lima Santiago de Pinho",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08604687895817407219"
     },
     "user_tz": 180
    },
    "id": "H4_MxZENkEt1",
    "outputId": "d03f08bd-f798-45ed-fa0e-7563d1c428fe"
   },
   "outputs": [],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNNHfwpLkEt1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 258,
     "status": "ok",
     "timestamp": 1642429773912,
     "user": {
      "displayName": "Alexandre Lima Santiago de Pinho",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08604687895817407219"
     },
     "user_tz": 180
    },
    "id": "jYorAP6XkEt1",
    "outputId": "0e5d9c5b-6702-4d4c-d9ec-5de0b0835efe"
   },
   "outputs": [],
   "source": [
    "movies['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExILvwM2kEt1"
   },
   "source": [
    "A nossa base de dados tem 50 mil linhas e levando em consideração que as críticas são sobre filmes diversos, a quantidade de palavras disponíveis nos textos será muito grande. Para economizar tempo de aula com processamneto dos textos e modelagem, iremos criar uma amostra com 10% da base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yDEz116DkEt1"
   },
   "outputs": [],
   "source": [
    "movies_sample = movies.sample(frac=0.1, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 373,
     "status": "ok",
     "timestamp": 1642429785213,
     "user": {
      "displayName": "Alexandre Lima Santiago de Pinho",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08604687895817407219"
     },
     "user_tz": 180
    },
    "id": "i17ZfHfakEt2",
    "outputId": "e96f583d-bcb8-407b-e957-9da3db50c243",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 327,
     "status": "ok",
     "timestamp": 1642098625899,
     "user": {
      "displayName": "Alexandre Lima Santiago de Pinho",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08604687895817407219"
     },
     "user_tz": 180
    },
    "id": "EG8ZM_KqkEt2",
    "outputId": "d55259d0-7894-4c93-e4b9-f19db4ceec50"
   },
   "outputs": [],
   "source": [
    "movies_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_0hnhvOkEt2"
   },
   "source": [
    "Agora iremos aplicar o nosso processamento dos textos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1EajLX23kEt2"
   },
   "outputs": [],
   "source": [
    "movies_sample[\"filtered_words\"] = movies_sample['text'].apply(lambda x: preprocessing(x))\n",
    "\n",
    "# Normalmente depois do processamento juntamos as palavras novamente em uma só string\n",
    "\n",
    "movies_sample['join_words'] = movies_sample['filtered_words'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1642430623980,
     "user": {
      "displayName": "Alexandre Lima Santiago de Pinho",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08604687895817407219"
     },
     "user_tz": 180
    },
    "id": "DqD89mJukEt2",
    "outputId": "4e884906-44e9-4d7e-d7c7-b59a2ca2c427"
   },
   "outputs": [],
   "source": [
    "movies_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NjYzoSNkEt2"
   },
   "source": [
    "##   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oj-COWchkEt3"
   },
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qecSzHqdkEt3"
   },
   "source": [
    "Com isso chegamos ao fim do pré-processamento, uma das etapas mais importantes de todo projeto de NLP!\n",
    "\n",
    "É importante ressaltar que a escolha das etapas de pré-processamento não é algo óbvio, dado que há muitas escolhas possíveis acerca do que se fazer para pré-processar os dados. Assim, o indicado é treinar diferentes modelos testando diferentes combinações das técnicas de pré-processamento, até que o melhor procedimento seja encontrado!\n",
    "\n",
    "**Nomenclatura**: o conjunto de mensagens pré-processadas é chamado de **Corpus**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZClnmRMckEt3"
   },
   "source": [
    "## Vocabulário\n",
    "\n",
    "O vocabulário do corpus nada mais é do que uma listagem das palavras individuais que aparecem no corpus. Para encontrar o vocabulário, basta contarmos a aparição de cada palavra isolada no corpus. Ao fim, teremos N palavras únicas que compõem nosso vocabulário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "executionInfo": {
     "elapsed": 1334539,
     "status": "ok",
     "timestamp": 1642099994710,
     "user": {
      "displayName": "Alexandre Lima Santiago de Pinho",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08604687895817407219"
     },
     "user_tz": 180
    },
    "id": "YloDdzIykEt3",
    "outputId": "c53ef39b-4ca8-4d38-c089-003202fec3ce"
   },
   "outputs": [],
   "source": [
    "vocabulario = []\n",
    "for frase in movies_sample['join_words']:\n",
    "    for palavra in frase.split():\n",
    "        \n",
    "        #não queremos palavras de uma única letra (pode acontecer devido ao stemming...)\n",
    "        if len(palavra) > 1:\n",
    "            if palavra not in [x[0] for x in vocabulario]:\n",
    "                vocabulario.append([palavra, 1])\n",
    "            else:\n",
    "                vocabulario[[x[0] for x in vocabulario].index(palavra)][1] += 1\n",
    "            \n",
    "print(\"\\nO vocabulário é formado por N =\", len(vocabulario), \"palavras!\")\n",
    "\n",
    "#a partir do vocabulário, crio um dataframe com a contagem\n",
    "vocab_count = pd.DataFrame({\"palavra\": [],\n",
    "                            \"count\": []})\n",
    "\n",
    "vocab_count[\"palavra\"] = pd.Series(vocabulario).apply(lambda x: x[0])\n",
    "vocab_count[\"count\"] = pd.Series(vocabulario).apply(lambda x: x[1])\n",
    "vocab_count = vocab_count.sort_values(\"count\", ascending=False)\n",
    "\n",
    "print(\"\\nTemos a seguir as 10 mais comuns, com as respectivas contagens:\")\n",
    "display(vocab_count.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umd7T-YSkEt3"
   },
   "source": [
    "##  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYWTm27dkEt3"
   },
   "source": [
    "## Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GK_PntJ2kEt3"
   },
   "source": [
    "**1)** Usando a base *spamham.csv*, faça o processamento dos textos aplicando as limpezas necessárias para tal. Tente levantar o vocabulário dos e-mails e print o top 10 palavras deste dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dpPtw29OkEt4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atqpXYenkEt4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TxwipUMfkEt4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nT47FfDDkEt4"
   },
   "source": [
    "**2)** Utilizando os dados de tweets vamos avaliar  tweets são de desastres ou não. Essa base é um dataset conhecido do Kaggle, onde vocês podem ter mais detalhes [clicando aqui](https://www.kaggle.com/c/nlp-getting-started/overview). Faça o processamento dos textos aplicando as limpezas necessárias para tal. Tente levantar o vocabulário dos e-mails e print o top 10 palavras deste dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kd2OmZeqkEt4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eRIAgxirkEt4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-KGuqQykEt4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Aula 11.1 - NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
